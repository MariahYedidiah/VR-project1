{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lyeakhHEqfx",
        "outputId": "73758c91-fda9-46ed-aea5-2a886dfc73ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.9057\n",
            "Random Forest Accuracy: 0.8805\n",
            "Epoch [1/100], Loss: 0.0000\n",
            "Epoch [11/100], Loss: 0.0013\n",
            "Epoch [21/100], Loss: 0.0002\n",
            "Epoch [31/100], Loss: 0.0177\n",
            "Epoch [41/100], Loss: 1.9169\n",
            "Epoch [51/100], Loss: 0.0456\n",
            "Epoch [61/100], Loss: 0.0000\n",
            "Epoch [71/100], Loss: 0.0000\n",
            "Epoch [81/100], Loss: 0.0000\n",
            "Epoch [91/100], Loss: 0.0000\n",
            "Neural Network Accuracy: 0.8911\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset images\n",
        "def load_images(image_paths, label):\n",
        "    images, labels = [], []\n",
        "    for img_path in image_paths:\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (128, 128))  # Resize to 128x128\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "        images.append(img.flatten())  # Flatten image\n",
        "        labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Define dataset paths\n",
        "mask_data_dir = \"/content/drive/My Drive/VR datasets/Face-Mask-Detection/dataset/with_mask/\"\n",
        "no_mask_data_dir = \"/content/drive/My Drive/VR datasets/Face-Mask-Detection/dataset/without_mask\"\n",
        "\n",
        "# Get image file paths\n",
        "mask_images = glob(os.path.join(mask_data_dir, \"*.jpg\"))\n",
        "no_mask_images = glob(os.path.join(no_mask_data_dir, \"*.jpg\"))\n",
        "\n",
        "# Load both datasets separately\n",
        "X_mask, y_mask = load_images(mask_images, label=1)\n",
        "X_no_mask, y_no_mask = load_images(no_mask_images, label=0)\n",
        "\n",
        "# Combine both datasets\n",
        "X = np.concatenate((X_mask, X_no_mask), axis=0)\n",
        "y = np.concatenate((y_mask, y_no_mask), axis=0)\n",
        "\n",
        "# Shuffle data\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "svm_model = SVC(kernel=\"rbf\", C=10, gamma=\"scale\")  # RBF kernel with adjusted C\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "\n",
        "# Import PyTorch only for Neural Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Neural Network\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(128 * 128, 224)\n",
        "        self.fc2 = nn.Linear(224, 224)\n",
        "        self.fc3 = nn.Linear(224, 224)\n",
        "        self.fc4 = nn.Linear(224, 224)\n",
        "        self.fc5 = nn.Linear(224, 224)\n",
        "        self.fc6 = nn.Linear(224, 2)  # Output layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)  # Dropout with 30% probability\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Dropout after first layer\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)  # Dropout after second layer\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.dropout(x)  # Dropout after third layer\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.dropout(x)  # Dropout after fourth layer\n",
        "        x = self.relu(self.fc5(x))\n",
        "        x = self.dropout(x)  # Dropout after fifth layer\n",
        "        x = self.fc6(x)  # No activation, since CrossEntropyLoss applies Softmax\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Learning rate reduced\n",
        "\n",
        "\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch%10==0:\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate model\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "print(f\"Neural Network Accuracy: {correct / total:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "from skimage.feature import hog  # NEW: Import HOG from scikit-image\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset images using HOG feature extraction\n",
        "def load_images(image_paths, label):\n",
        "    images, labels = [], []\n",
        "    for img_path in image_paths:\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (128, 128))  # Resize to 128x128\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "        # Extract HOG features\n",
        "        features = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
        "                       cells_per_block=(2, 2), block_norm='L2-Hys',\n",
        "                       visualize=False, feature_vector=True)\n",
        "        images.append(features)\n",
        "        labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Define dataset paths\n",
        "mask_data_dir = \"/content/drive/My Drive/VR datasets/Face-Mask-Detection/dataset/with_mask/\"\n",
        "no_mask_data_dir = \"/content/drive/My Drive/VR datasets/Face-Mask-Detection/dataset/without_mask\"\n",
        "\n",
        "# Get image file paths\n",
        "mask_images = glob(os.path.join(mask_data_dir, \"*.jpg\"))\n",
        "no_mask_images = glob(os.path.join(no_mask_data_dir, \"*.jpg\"))\n",
        "\n",
        "# Load both datasets separately\n",
        "X_mask, y_mask = load_images(mask_images, label=1)\n",
        "X_no_mask, y_no_mask = load_images(no_mask_images, label=0)\n",
        "\n",
        "# Combine both datasets\n",
        "X = np.concatenate((X_mask, X_no_mask), axis=0)\n",
        "y = np.concatenate((y_mask, y_no_mask), axis=0)\n",
        "\n",
        "# Shuffle data\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "svm_model = SVC(kernel=\"rbf\", C=10, gamma=\"scale\")  # RBF kernel with adjusted C\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "\n",
        "# Import PyTorch only for Neural Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Determine input dimension from HOG features (all images now have the same length)\n",
        "INPUT_DIM = X_train.shape[1]\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Neural Network with 5 hidden layers, 224 neurons each, and dropout\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(INPUT_DIM, 224)\n",
        "        self.fc2 = nn.Linear(224, 224)\n",
        "        self.fc3 = nn.Linear(224, 224)\n",
        "        self.fc4 = nn.Linear(224, 224)\n",
        "        self.fc5 = nn.Linear(224, 224)\n",
        "        self.fc6 = nn.Linear(224, 2)  # Output layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)  # Dropout with 30% probability\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Dropout after first layer\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)  # Dropout after second layer\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.dropout(x)  # Dropout after third layer\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.dropout(x)  # Dropout after fourth layer\n",
        "        x = self.relu(self.fc5(x))\n",
        "        x = self.dropout(x)  # Dropout after fifth layer\n",
        "        x = self.fc6(x)  # No activation, since CrossEntropyLoss applies Softmax\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate model\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "print(f\"Neural Network Accuracy: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElPEJcXxp37b",
        "outputId": "6dbe2a58-301a-4ccb-f559-a4354a13f66a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "SVM Accuracy: 0.9336\n",
            "Random Forest Accuracy: 0.8805\n",
            "Epoch [1/100], Loss: 0.0029\n",
            "Epoch [11/100], Loss: 0.0000\n",
            "Epoch [21/100], Loss: 0.0000\n",
            "Epoch [31/100], Loss: 0.0000\n",
            "Epoch [41/100], Loss: 0.0000\n",
            "Epoch [51/100], Loss: 0.0000\n",
            "Epoch [61/100], Loss: 0.0000\n",
            "Epoch [71/100], Loss: 0.0000\n",
            "Epoch [81/100], Loss: 0.0000\n",
            "Epoch [91/100], Loss: 0.0000\n",
            "Neural Network Accuracy: 0.9110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qikgrYaDqARC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen models effectively handle high-dimensional data and mitigate the impact of irrelevant features. Since this is a supervised learning task, KNN and GMM were not considered. Although logistic regression, a linear classifier, was included, it exhibited relatively poor performance. Traditional ML methods, such as SVM and random forests, showed varying results, with random forests demonstrating greater robustness due to its ensemble nature. As expected, the neural network outperformed traditional models, leveraging its ability to learn hierarchical representations.Also,intead of simply flattening the image ,if we use handcrafted features like HoG,performance improves"
      ],
      "metadata": {
        "id": "QjlmzbYuKqf7"
      }
    }
  ]
}